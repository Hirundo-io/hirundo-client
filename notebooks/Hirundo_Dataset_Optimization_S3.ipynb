{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Hirundo-io/hirundo-client/blob/clnt-9-add-jupyter-notebooks-to-github/notebooks/Hirundo_Dataset_Optimization_S3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOHjhp9ed6LM"
   },
   "source": [
    "# How to use Hirundo's Dataset Optimization (S3)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Let's start with a simple example using a dataset we've prepared and uploaded to an AWS S3 bucket.\n",
    "\n",
    "## AWS S3 bucket example\n",
    "\n",
    "1. We import `os` and `google.colab`'s `userdata` to get our secrets and assign them to environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHGkhvIig3MS"
   },
   "outputs": [],
   "source": [
    "%pip install hirundo\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata  # type: ignore  In Google Colab, this will work\n",
    "\n",
    "    os.environ[\"AWS_ACCESS_KEY\"] = userdata.get(\"AWS_ACCESS_KEY\")\n",
    "    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = userdata.get(\"AWS_ACCESS_KEY\")\n",
    "    os.environ[\"API_HOST\"] = userdata.get(\"API_HOST\")\n",
    "    os.environ[\"API_KEY\"] = userdata.get(\"API_KEY\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\n",
    "        \"You are not in Google Colab, so you need to set AWS_ACCESS_KEY, AWS_SECRET_ACCESS_KEY, API_HOST, \"\n",
    "        \"and API_KEY environment variables manually.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qSUfS1liadM"
   },
   "source": [
    "2. We import the `OptimizationDataset` class, as well as the `LabelingType` enum, the `StorageConfig` (to indicate where the dataset files are saved) class, `the StorageTypes` enum, and the `StorageS3` storage class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RhDZX-KlbUIT"
   },
   "outputs": [],
   "source": [
    "from hirundo import (\n",
    "    HirundoCSV,\n",
    "    LabelingType,\n",
    "    OptimizationDataset,\n",
    "    StorageConfig,\n",
    "    StorageS3,\n",
    "    StorageTypes,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KhP3Cw6gqlB"
   },
   "source": [
    "3. First we create the `OptimizationDataset` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vE33z9_egk3u"
   },
   "outputs": [],
   "source": [
    "s3_storage = StorageS3(\n",
    "    bucket_url=\"s3://hirundo-open-source-datasets\",\n",
    "    region_name=\"il-central-1\",\n",
    "    access_key_id=os.environ[\"AWS_ACCESS_KEY\"],\n",
    "    secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    ")\n",
    "test_dataset = OptimizationDataset(\n",
    "    name=\"AWS-test-OD-BDD-validation-dataset\",\n",
    "    labeling_type=LabelingType.OBJECT_DETECTION,\n",
    "    storage_config=StorageConfig(\n",
    "        name=\"AWS-open-source-datasets\",\n",
    "        type=StorageTypes.S3,\n",
    "        s3=s3_storage,\n",
    "    ),\n",
    "    labeling_info=HirundoCSV(\n",
    "        csv_url=s3_storage.get_url(\n",
    "            \"/bdd100k_subset_1000_hirundo.zip/bdd100k/bdd100k.csv\"\n",
    "        ),\n",
    "        #  csv_url=\"s3://hirundo-open-source-datasets/bdd100k_val_hirundo.zip/bdd100k/bdd100k.csv\",\n",
    "    ),\n",
    "    data_root_url=s3_storage.get_url(\"/bdd100k_subset_1000_hirundo.zip/bdd100k/\"),\n",
    "    #  data_root_url=\"s3://hirundo-open-source-datasets/bdd100k_val_hirundo.zip/bdd100k\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJhmScbels65"
   },
   "source": [
    "4. Now that we have created our dataset, we can launch a dataset optimization run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WqZQzEX6d5id"
   },
   "outputs": [],
   "source": [
    "run_id = test_dataset.run_optimization(replace_dataset_if_exists=True)\n",
    "print(\"Running optimization. Run ID is \", run_id)\n",
    "test_dataset.check_run()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNbLUwIEwJ3Pfh0KL/WRz7t",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
