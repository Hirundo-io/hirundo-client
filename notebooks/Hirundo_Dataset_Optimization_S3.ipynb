{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Hirundo-io/hirundo-client/blob/clnt-9-add-jupyter-notebooks-to-github/notebooks/Hirundo_Dataset_Optimization_S3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOHjhp9ed6LM"
   },
   "source": [
    "# How to use Hirundo's Dataset Optimization (S3)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Let's start with a simple example using a dataset we've prepared and uploaded to an AWS S3 bucket.\n",
    "\n",
    "## AWS S3 bucket example\n",
    "\n",
    "1. We import `os` and `google.colab`'s `userdata` to get our secrets and assign them to environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHGkhvIig3MS"
   },
   "outputs": [],
   "source": [
    "%pip install hirundo\n",
    "import os\n",
    "\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY\"] = userdata.get(\"AWS_ACCESS_KEY\")\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = userdata.get(\"AWS_ACCESS_KEY\")\n",
    "os.environ[\"API_HOST\"] = userdata.get(\"API_HOST\")\n",
    "os.environ[\"API_KEY\"] = userdata.get(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qSUfS1liadM"
   },
   "source": [
    "2. We import the `OptimizationDataset` class, as well as the `LabelingType` enum, the `StorageIntegration` (to indicate where the dataset files are saved) class, `the StorageTypes` enum, and the `StorageS3` storage class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RhDZX-KlbUIT"
   },
   "outputs": [],
   "source": [
    "from hirundo import (\n",
    "    LabelingType,\n",
    "    OptimizationDataset,\n",
    "    StorageIntegration,\n",
    "    StorageS3,\n",
    "    StorageTypes,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KhP3Cw6gqlB"
   },
   "source": [
    "3. First we create the `OptimizationDataset` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vE33z9_egk3u"
   },
   "outputs": [],
   "source": [
    "test_dataset = OptimizationDataset(\n",
    "    name=\"AWS-test-OD-BDD-validation-dataset\",\n",
    "    labeling_type=LabelingType.ObjectDetection,\n",
    "    storage_integration=StorageIntegration(\n",
    "        name=\"AWS-open-source-datasets\",\n",
    "        type=StorageTypes.S3,\n",
    "        s3=StorageS3(\n",
    "            bucket_url=\"s3://hirundo-open-source-datasets\",\n",
    "            region_name=\"il-central-1\",\n",
    "            access_key_id=os.environ[\"AWS_ACCESS_KEY\"],\n",
    "            secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        ),\n",
    "    ),\n",
    "    root=\"/bdd100k_val_hirundo.zip/bdd100k\",\n",
    "    dataset_metadata_path=\"bdd100k.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJhmScbels65"
   },
   "source": [
    "4. Now that we have created our dataset, we can launch a dataset optimization run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WqZQzEX6d5id"
   },
   "outputs": [],
   "source": [
    "run_id = test_dataset.run_optimization()\n",
    "print(\"Running optimization. Run ID is \", run_id)\n",
    "test_dataset.check_run()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNbLUwIEwJ3Pfh0KL/WRz7t",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
