{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Hirundo-io/hirundo-client/blob/clnt-9-add-jupyter-notebooks-to-github/notebooks/Create_BDD100k_dataset_HuggingFace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_LpV5xrmW9e"
   },
   "source": [
    "# How to create a Hirundo dataset (HuggingFace)\n",
    "\n",
    "--\n",
    "\n",
    "0. Install `pandas` and `tqdm` and set `huggingface_account`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gTmTQZEFQ57J",
    "outputId": "fdec8b65-8c65-4979-fb3f-528a777f3aac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "import os\n",
    "\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
    "\n",
    "huggingface_account = \"hirundo-io\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_EG50TkQ8E0"
   },
   "source": [
    "1. Import `tempfile` to create a temporary directory, and import `requests`, `zipfile`, `io`, & `hashlib` to download and unzip BDD100k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PnT-el8nQ-MG"
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import io\n",
    "import tempfile\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "\n",
    "bdd100k_temp_dir = tempfile.TemporaryDirectory()\n",
    "bdd100k_temp_dir_name = bdd100k_temp_dir.name\n",
    "bdd100k_temp_dir_path = Path(bdd100k_temp_dir_name)\n",
    "\n",
    "\n",
    "def get_zip_check_md5_and_unzip(url: str, target_folder: str, check_md5=False):\n",
    "    data_request_result = requests.get(\n",
    "        url,\n",
    "        timeout=900.0,\n",
    "    )\n",
    "    data_bytes = data_request_result.content\n",
    "    if check_md5:\n",
    "        data_verify_md5 = requests.get(\n",
    "            f\"{url}.md5\",\n",
    "            timeout=30.0,\n",
    "        ).text.split(\" \")[0]\n",
    "        data_actual_md5 = hashlib.md5(data_bytes).hexdigest()\n",
    "        if data_verify_md5 != data_actual_md5:\n",
    "            raise ValueError(\n",
    "                f\"ZIP download failed. {data_verify_md5} != {data_actual_md5}. Try again\"\n",
    "            )\n",
    "    data_zip = zipfile.ZipFile(io.BytesIO(data_bytes))\n",
    "    data_zip.extractall(target_folder)\n",
    "\n",
    "\n",
    "get_zip_check_md5_and_unzip(\n",
    "    \"https://dl.cv.ethz.ch/bdd100k/data/100k_images_val.zip\",\n",
    "    bdd100k_temp_dir_name,\n",
    "    check_md5=True,\n",
    ")\n",
    "get_zip_check_md5_and_unzip(\n",
    "    \"https://dl.cv.ethz.ch/bdd100k/data/bdd100k_det_20_labels_trainval.zip\",\n",
    "    bdd100k_temp_dir_name,\n",
    ")\n",
    "#  ⬆️ as per: https://doc.bdd100k.com/download.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCbIfyb0SEOL"
   },
   "source": [
    "2. Import `Path` from `pathlib`, `json`, `pandas` and `tqdm` to create dataset DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kg_jEsE3RGs2"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ye7piGx8SLJ9"
   },
   "source": [
    "3. Create Hirundo CSV for dataset, with `tqdm` to track progress, from source JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rz6zqU1pHvIT"
   },
   "outputs": [],
   "source": [
    "df_rows = []\n",
    "\n",
    "bdd100k_hirundo_temp_dir = tempfile.TemporaryDirectory()\n",
    "bdd100k_hirundo_temp_dir_name = bdd100k_hirundo_temp_dir.name\n",
    "\n",
    "bdd100k_hirundo_temp_dir_path = Path(bdd100k_hirundo_temp_dir_name)\n",
    "with open(bdd100k_temp_dir_path / \"bdd100k/labels/det_20/det_val.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for sample in tqdm(data, total=len(data), desc=\"Loading BDD100K validation set\"):\n",
    "    if sample[\"labels\"] is not None:\n",
    "        for i, label in enumerate(sample[\"labels\"]):\n",
    "            df_rows.append(\n",
    "                {\n",
    "                    \"image_path\": sample[\"name\"],\n",
    "                    \"bbox_id\": str(\n",
    "                        i\n",
    "                    ),  # Box index in image used for box ID (since no unique ID is provided)\n",
    "                    \"label\": label[\"category\"],\n",
    "                    \"xmin\": int(label[\"box2d\"][\"x1\"]),\n",
    "                    \"ymin\": int(label[\"box2d\"][\"y1\"]),\n",
    "                    \"xmax\": int(label[\"box2d\"][\"x2\"]),\n",
    "                    \"ymax\": int(label[\"box2d\"][\"y2\"]),\n",
    "                }\n",
    "            )\n",
    "\n",
    "df = pd.DataFrame(df_rows)\n",
    "df.to_csv(bdd100k_hirundo_temp_dir_path / \"bdd100k.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxQCfz7FR_z_"
   },
   "source": [
    "4. Create a ZIP of the dataset to upload to HuggingFace.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q9jw8DkwSpak"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "relative_path = \"bdd100k/images\"\n",
    "(bdd100k_hirundo_temp_dir_path / \"bdd100k\").mkdir()\n",
    "shutil.copytree(\n",
    "    bdd100k_temp_dir_path / relative_path, bdd100k_hirundo_temp_dir_path / relative_path\n",
    ")\n",
    "\n",
    "bdd_zip = shutil.make_archive(\n",
    "    base_name=\"bdd100k_val_hirundo\", format=\"zip\", root_dir=bdd100k_temp_dir_path\n",
    ")\n",
    "bdd_zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAQ6dC-OSptk"
   },
   "source": [
    "4. Upload to HuggingFace with `datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Hh2yPkITBtL"
   },
   "outputs": [],
   "source": [
    "%pip install huggingface_hub\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "bdd_zip_filename = Path(bdd_zip).name\n",
    "\n",
    "api = HfApi()\n",
    "if not api.repo_exists(f\"{huggingface_account}/bdd100k-val\"):\n",
    "    api.create_repo(\n",
    "        f\"{huggingface_account}/bdd100k-val\",\n",
    "        repo_type=\"dataset\",\n",
    "    )\n",
    "api.upload_file(\n",
    "    path_or_fileobj=bdd_zip,\n",
    "    path_in_repo=bdd_zip_filename,\n",
    "    repo_id=f\"{huggingface_account}/bdd100k-val\",\n",
    "    repo_type=\"dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfa6i34pSvBO"
   },
   "source": [
    "6. Get `BDD100k` class list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxQi_Z8iSutj"
   },
   "outputs": [],
   "source": [
    "bdd100k_classes = df[\"label\"].unique()\n",
    "'\"' + '\", \"'.join(bdd100k_classes) + '\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TdFBuBqSnTt"
   },
   "source": [
    "7. Cleanup dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2RcNa_oHR_QP"
   },
   "outputs": [],
   "source": [
    "bdd100k_temp_dir.cleanup()\n",
    "bdd100k_hirundo_temp_dir.cleanup()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMfoIWUmqYeSJxPkHBlzuIx",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
